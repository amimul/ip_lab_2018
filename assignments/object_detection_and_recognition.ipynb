{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 : Object Detection and Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "from utils import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Person Detection \n",
    "\n",
    "In this section, we will return to the HOG features from the last Chapter. As we said before, HOG was proposed as a useful feature for human detection. If you reach this point, you may have noticed that Template matching may not be the best option for this. Imagine how difficult it would be to create a template for any human-shaped structure that you would like to detect as a human in a scene. Instead of that, you will train a Linear Classifier from scratch. \n",
    "\n",
    "Section objectives:\n",
    "\n",
    "In this section, since you know the basics of HOG, you will use OpenCV's implementation to extract the HOG's features of the curated INRIA's Persons dataset to train an SVM Linear classifier (https://en.wikipedia.org/wiki/Support_vector_machine). For this, instead of using OpenCV's (already trained) classifier, we will use the Scikit-learn Machine Learning library (http://scikit-learn.org). Which is one of the most used machine learning libraries around this days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    " This dataset was collected as part of the research work on detection of upright people in images and video. The research is described in detail in the CVPR 2005 paper _Histograms of Oriented Gradients for Human Detection_. The full dataset is about ~1 GB and contains several thousands of pedestrian images. \n",
    " \n",
    "For your convenience, the dataset is already separated into two sets: \n",
    "* \" **_Positives_**\" which are all the images containing at least one person. \n",
    "* \"**_Negatives_**\" any kind of non-human shaped objects images.\n",
    "\n",
    "In addition, the data is already separated in a **training** and **testing** set (seriously, it cannot be more conveniently done).\n",
    "\n",
    "Link:  <a ref=ftp://ftp.inrialpes.fr/pub/lear/douze/data/INRIAPerson.tar> ftp://ftp.inrialpes.fr/pub/lear/douze/data/INRIAPerson.tar </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Data\n",
    "# The following functions will download the data for you and uncompress it\n",
    "\n",
    "url = ' ftp://ftp.inrialpes.fr/pub/lear/douze/data/'\n",
    "data_root = '../data/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"\n",
    "  Downloads a file if not present, and make sure it's the right size!.\n",
    "  If there's a file with the same name, the function will not try to \n",
    "  download the data set again!\n",
    "  \"\"\"\n",
    "\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "    \n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename, 'This may take a while. Please wait.') \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'The file ' + dest_filename + 'already exist but seems corrupted. Delete it or download it from the browser!')\n",
    "  return dest_filename\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  \"\"\"\n",
    "  Uncompress the data set for you\n",
    "  \"\"\"\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s dataset (seems to be) already present.\\nSkipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "    \n",
    "  print(\"All setup.\")\n",
    "\n",
    "\n",
    "dataset_tar = maybe_download('INRIAPerson.tar', 1016094720)\n",
    "\n",
    "dataset = maybe_extract(dataset_tar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have problems running the functions above, you can download manually the dataset from the link provided above and uncompress it in your _data_ directory.\n",
    "<br>\n",
    "\n",
    "Once you have the data, you will now process each image on both: _positive_ and _negative_ folders using the OpenCV HOG Descriptor implementation: \n",
    "\n",
    "https://docs.opencv.org/2.4/modules/gpu/doc/object_detection.html\n",
    "\n",
    "\n",
    "Your job: Using the skeleton provided below, for each image in the \"pos\" and \"neg\" folder of the *training* set:\n",
    "\n",
    "* Compute the hog feature vector using the parameters provided below. \n",
    "    * The length of each individual feature vector should be of 16800. Derive bellow why is of this size. (Hint: Imagine that you have an image of size (32,32) which would be the size of this case?).\n",
    "   \n",
    "* Append the feature vector to the _training_feature_ list.\n",
    "    * The total size of this list should be of (16800, number of images in your training set). \n",
    "\n",
    "* For each image add to the _label_ vector a 1 if it's positive or 0 if it's negative. The final length of the _labels_ should be the number of images in your full training set.\n",
    "\n",
    "WARNING: The dataset may contain corrupted images. Be sure, inside your code, to check if the image was loaded properly. Otherwise, you will get either trash features or execution errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training set:\n",
    "\n",
    "#Positive folder:\n",
    "pos_im_path = \"../data/INRIAPerson/train_64x128_H96/pos/\"\n",
    "#Negative folder:\n",
    "neg_im_path = \"../data/INRIAPerson/train_64x128_H96/neg/\"\n",
    "\n",
    "# Window size: this specifies the size of the input image (remember to scale the input to this size!)\n",
    "winSize     = (64,128)\n",
    "\n",
    "# Size on pixels of each block (remember that a block contains a set of CELLS)\n",
    "blockSize   = (16,16)\n",
    "\n",
    "# The separation between each block. If this value is less than the block size, \n",
    "# there will be overlapping blocks. \n",
    "block_stride = (8,8)\n",
    "\n",
    "# The size of each CELL. Each cell computes one histogram. The cells should FIT inside a block.\n",
    "cellSize = (4,4)\n",
    "\n",
    "# Number of bins for each histogram.\n",
    "nbins = 10\n",
    "\n",
    "# hog is an instance taht contains the info and is able to compute the feature vector.\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,block_stride,cellSize, nbins)\n",
    "\n",
    "# list to save ALL the features.\n",
    "training_features = []\n",
    "\n",
    "# Auxiliary array to label each features if it comes from a \"positive\"(1) or \"negative\" (0) image.\n",
    "labels = []\n",
    "\n",
    "print (\"Calculating the descriptors for the positive samples and saving them\")\n",
    "\n",
    "# CODE here.{\n",
    "\n",
    "#}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have this features computed and saved in a feature matrix, you are pretty much set to train a classifier. \n",
    "\n",
    "scikit-learn provides all the implementation needed to train a linear classifier with no more than 2 lines of code! (Which is awesome and sad at the same time). \n",
    "\n",
    "For this exercise, you will use a Linear Support Vector Classifier. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "Please take the time to read the implementation details, but more importantly, the examples and theory provided in the documentation. If you are not interested in knowing stuff, you can jump right away to use the implemented functions in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the classifier.\n",
    "\n",
    "# Check the dimentions fits\n",
    "print (np.array(training_features).shape,len(labels))\n",
    "# (3634, 16800, 1) 3634\n",
    "\n",
    "\n",
    "# create a LINEAR classifier instance here (LinearSVC): \n",
    "\n",
    "\n",
    "#train the classifier (LinearSVC.fit).\n",
    "print (\"Training a Linear SVM Classifier\")\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the classifier -which a fancy way to say that you fitted a 1-D vector of coefficients!- you can use this Support Vector Machine to *classify* if, given an input * HOG feature vector* (with strictly the same dimensions as your training data), it came from an image with a human-shaped form in it (prediction = 1), or not (prediction = 0).\n",
    "\n",
    "*Your job*:\n",
    "\n",
    "For each image in the test folders: \n",
    "* Compute the HOG feature vector.\n",
    "* Predict/classify the vector as positive (1) or negative (0); Hint: LinearSCV.predict(...)\n",
    "* Compute the estimation error for the negative and positive images _separetely_.\n",
    "* Compute and report the F1-score https://en.wikipedia.org/wiki/F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_neg_path = \"../data/INRIAPerson/test_64x128_H96/neg/\"\n",
    "test_pos_path = \"../data/INRIAPerson/test_64x128_H96/pos/\"\n",
    "\n",
    "print (\"Estimating the test data [Negative samples]\")\n",
    "\n",
    "negative_prediction = []\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Estimating the test data [Positive samples]\")\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic classifier above can (should) achieve a success rate of 89% for the positive and less than 2% error for the negative images respectively. \n",
    "\n",
    "Can you tweak the HOG parameters to improve a little bit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, from the description above, what we created is no more than a *classifier* for only 2 classes (binary classifier): human(human-shaped) or not. In order to create a functional Person detector for arbitrary images or video sequences, some engineering techniques (heuristics) need to be implemented. \n",
    "\n",
    "Pretty much as in template matching, in order to find a person in an arbitrary image you will need to: \n",
    "\n",
    "* Slide your classifier over the full area of the image.\n",
    "* Detect possible matchings. \n",
    "* Report them as positive or negatives\n",
    "* And optionally, repeat the procedure above in different scales, to assure multiscale detection!.\n",
    "\n",
    "The procedure is nicely depicted in the image below for face detection.\n",
    "\n",
    "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2015/03/sliding-window-animated-adrian.gif\"> \n",
    "</img>\n",
    "_Image taken from: https://www.pyimagesearch.com_\n",
    "\n",
    "For now, we will leave those implementation details for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Face Recognition\n",
    "\n",
    "In this section we will see how it is possible to recognize objects. The study case will target face regognition. Two approaches will be explored, `Eigen Faces` and `Fisher Faces` respectively. These methods are both based dimensional reduction technics listed below:\n",
    "\n",
    "- Principal Component Analysis ([*PCA*](http://www.utdallas.edu/~herve/abdi-awPCA2010.pdf))\n",
    "- Linear Discriminant Analysis (*LDA*)\n",
    "\n",
    "In general, in order to train a recognizer, several steps are needed and can be grouped as follow:\n",
    "\n",
    "- Data preparation\n",
    "- Recognizer training\n",
    "- System validation\n",
    "\n",
    "It is important for the validation step to ensure that the system tested with **unseen** data. By unseen data we mean data that have not been used during the training phase, this will ensure a fair performance assessment without biais. However this does not garantie that the system will *generalize* well to other dataset.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Data preparation covers various aspect of pre-processing step for training a system. At first, the images need to be splitted into two disctinct subsets thant will be used for `training` and `testing`. In our experiment the dataset used is the *Yale dataset version B*, it includes a total of 38 different identity (*i.e. subject*) each having 20 images undergoing different illumination condition for a total of 760 samples. The splitt will be done by chosing randomly samples from each subjects and placed into the corresponding subset, special care need to be taken in order to avoid having the same example multiple time.\n",
    "\n",
    "The first task is to gather the labels and the images that will be used to train the system. One solution is to store these information into a dictionary where the identity is the key and the pathes to the images for this subject are the values. \n",
    "\n",
    "** Your Answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Scan for images in a given `path` and extract the label as well\n",
    "\n",
    "    :param path:    Path where YaleB dataset is stored\n",
    "    :return:        Dictionary storing the ID and a list of images for this ID\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    # Scan folder\n",
    "    dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    # Query images for each subject and extract the subject's ID\n",
    "    \n",
    "    # Your code goes here ...\n",
    "    \n",
    "\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_dataset(os.path.join('..', 'data', 'yaleB'))\n",
    "assert(len(data) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all the images and labels have been gathered, the next step is to split into two subsets, the train set and test set. The training set will be composed of $75\\%$ of the images of each subject and the remaining $25\\%$ will be used as test set.\n",
    "\n",
    "Once again the two subsets information will be stored into two separates dictionary similar to what has been done earlier.\n",
    "\n",
    "** Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, ratio):\n",
    "    \"\"\"\n",
    "    Split randomly a dataset into two subsets. The ratio provides the distribution for each subset\n",
    "\n",
    "    :param data:    Overall dataset\n",
    "    :param ratio:   Split ratio\n",
    "    :return:        Two dictionaries, train/test\n",
    "    \"\"\"\n",
    "    train = {}\n",
    "    test = {}\n",
    "    \n",
    "    # Your code goes here ...\n",
    "    \n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    \n",
    "\n",
    "    # return subsets    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_dataset(data, 0.75)\n",
    "assert(len(train) == 38)\n",
    "assert(len(test) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these two subsets, we can load the images and extract features from them. In this case, the pixel intensity will be used as a feature, therefore for an image $I \\in \\mathbb{R}^{ w \\times h}$ the feature vector will have a size of $wh$. This value can be quite large very easily, therefore all images will be downsampled by a factor of $2$.\n",
    "\n",
    "All the training samples will be concatenated into a single matrix where each row is an image (*i.e. flattened*) with dimensions $N \\times K$ where $N$ is the number of samples and $K$ is the dimension of a single image, $K = \\frac{wh}{4}$. The corresponding labels will also be concatenated into a single vector of dimension $N \\times 1$.\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(subset):\n",
    "    \"\"\"\n",
    "    Load images into one single matrix where each row is one single image (flattened). The final dimensions is [N x K]\n",
    "    where N is the numper of samples available and K is the number of pixel in one image. The original image is first\n",
    "    downspample by a factor of 2\n",
    "\n",
    "    Labels are also exported into a single vector of dimensions [N x 1]\n",
    "\n",
    "    :param subset: Dictionary storing labels/images\n",
    "    :return:       Data matrix and label vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # data = ...\n",
    "    # label = ...\n",
    "    \n",
    "    # Your code goes here ...\n",
    "    \n",
    "    \n",
    "\n",
    "    # Done\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load training images into memery\n",
    "train_img, train_label = load_images(train)\n",
    "# Sanity check\n",
    "assert(train_img.shape[0] == train_label.size)\n",
    "# Output number of samples\n",
    "print(\"There is a total of {} samples for the training set\".format(train_label.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Eigenfaces\n",
    "\n",
    "To perform recognition, all pixel's intensities are used as a feature vector. The dimension of these descriptors will be large, therefore a *clever* representation of the data, called subspace, is needed. \n",
    "\n",
    "This subspace is computed using *Principal Component Analysis* method in order to extract meaningfull information and reduce the dimension of the problem. The *PCA* approach is completely unsupervised and extracts the directions, or *basis*, where the variation in the data is the largest inside the feature space. \n",
    "\n",
    "Since we are interested in the variation in the data, the first step is to remove the commmon information present in all samples by subtracting the **average face**. The average is computed using all training samples $I_i$ as follow:\n",
    "\n",
    "$$\n",
    "\\bar{\\boldsymbol{I}} = \\frac{1}{N_t} \\sum_{i=0}^{N_t} \\boldsymbol{I}_i\n",
    "$$\n",
    "\n",
    "where $N_t$ is the total number of training samples and $I_i$ is a specific training sample. Then each samples $I_i$ are normalized as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi_i} = \\boldsymbol{I}_i - \\bar{\\boldsymbol{I}}\n",
    "$$\n",
    "\n",
    "With all samples normalized, we need to find a set of orthogonal basis which best explains the distribution of our data. To do so we compute the eigendecomposition of the covariance matrix of the normalized samples.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{C} = \\frac{1}{N_t - 1} \\sum_{i}^{N_t} \\boldsymbol{\\phi}_i\\boldsymbol{\\phi}_i^{\\top} = \\frac{1}{N_t - 1} \\boldsymbol{\\Phi\\Phi}^{\\top}, \\quad \\text{where } \\Phi = \\left[\\boldsymbol{\\phi}_0, \\dots, \\boldsymbol{\\phi}_{N} \\right]\n",
    "$$\n",
    "\n",
    "Find the eigenvectors $u_k$ and the eigenvalues $\\lambda_k$. \n",
    "\n",
    "So far the dimensions of the problem have not been reduced. Moreover the size of the covariance matrix will be $K \\times K$ with $K = \\frac{wh}{4}$. Therefore we will find $K$ eigenvectors representing the variation in our data. To reduce the dimension we will select only the eigenvectors that contribute the most to the variation and dropping the one with little influence. Doing so will reduce the dimension of the subspace to $K \\times K_m$.\n",
    "\n",
    "The question is how to properly determine this $K_m$ value. It can be done by using the eigenvalues computed earlier. These values are representing the energy each vector contribute to. Therefore it is possible to determine the number of bases to select in order to retain a certain amount of energy.\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{k=0}^{K_m}\\lambda_k}{\\sum_{i=0}^{K}\\lambda_i} < \\Theta\n",
    "$$\n",
    "\n",
    "Where $\\Theta$ represents the amount of energy to retain, which usually is around $95\\%$ but can vary depending on the application. \n",
    "Finally the subspace is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{U} = \\left[\\boldsymbol{u}_0, \\dots, \\boldsymbol{u}_{K_m}\\right], \\quad \\boldsymbol{U} \\in \\mathbb{R}^{K \\times K_m}\n",
    "$$\n",
    "\n",
    "In practice, the PCA decomposition is computed using `sklearn.decomposition.PCA`, more information can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "\n",
    "For now, compute the face subspace on the training data by retaining $95\\%$ of the variance present in the training data.\n",
    "Once the subspace is computed, display the first $8$ modes or *eigenfaces* and comment on what you see, what do you think are the limitations of such approach?\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute EigenFaces with PCA while keeping 95% of the variance\n",
    "retain_var = 0.95\n",
    "# Compute Eigenfaces\n",
    "pca = # Code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first four eigenfaces\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 7))\n",
    "for k in range(8):\n",
    "    # Define eigenface\n",
    "    \n",
    "    # Your code goes here ...\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our fresh subspace computed we can try to visualize if it separates properly the different subjects. Having a proper plot of an $N$ dimensional space is not feasible, however, we can use only a few components (*i.e. 2 or 3*) of our projected samples to visualize them. \n",
    "\n",
    "\n",
    "What we want is a subspace that is able to separates and clusters properly each subject in order to avoid miss recognition. The code snippet below shows the $3^{rd}$ and $4^{th}$ components on a $2D$ plane.\n",
    "\n",
    "What do you think about it, do we have clean inter-subject separation?\n",
    "\n",
    "** Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Poject data onto subspace\n",
    "proj_train = pca.transform(train_img)\n",
    "\n",
    "# Visualize \n",
    "# Colors for distinct individuals\n",
    "colors = LabelEncoder().fit_transform(train_label.ravel())\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(proj_train[:, 2], proj_train[:, 3], c=colors)\n",
    "plt.xlabel('PC2')\n",
    "plt.ylabel('PC3')\n",
    "plt.title('Trainset clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our learned subspace we need to defined the corresponding representation for the training samples. This can be done by projecting them into the eigen subspace (*i.e. eigenface*) as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_i = \\boldsymbol{U}^\\top \\boldsymbol{\\phi}_i\n",
    "$$\n",
    "\n",
    "In the training set, there are multiple samples avaible for each subject. Their projection won't be exactly the same, therefore we need to have a more generic representation of each person. To do so, we average all representation of the specific person to have his general representation.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Omega}_k = \\frac{1}{N_k} \\sum_{p} \\boldsymbol{\\omega}_p\n",
    "$$\n",
    "\n",
    "where $N_k$ is the number of samples for subject $k$, $\\boldsymbol{\\omega}_p$ represents the projected samples of subject $k$ respectively. In our case, $k$ goes from $0$ to $37$.\n",
    "\n",
    "Now implement the function below that computes each subject's centroid, and returned their corresponding labels as well.\n",
    "\n",
    "** Your answer **\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(data, trsfrm, label):\n",
    "    \"\"\"\n",
    "    Given a list of training samples, compute the centroids of each uniques labels\n",
    "\n",
    "    :param data:    Matrix with all feature vectors stored as row\n",
    "    :param trsfrm:  Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param label:   List of corresponding labels\n",
    "    :return:        Centroids, unique labels\n",
    "    \"\"\"\n",
    "\n",
    "    # extract unique label\n",
    "    unique_lbl = np.unique(label)\n",
    "\n",
    "    # Your code goes here ...\n",
    "    \n",
    "\n",
    "    # Done\n",
    "    return centroids, unique_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define centroids\n",
    "train_centroids, train_centroid_label = compute_centroids(train_img, pca, train_label)\n",
    "assert(train_centroids.shape[0] == 38)\n",
    "assert(train_centroid_label.shape[0] == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've learned a face representation and computed the descriptor for the training samples. Now we can use them to recognize a face. To do so the first step is to bring the *new* sample into our face *subspace* by projection, similarly  to what has been done before:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_n = \\boldsymbol{U}^{\\top}(\\boldsymbol{I}_n - \\bar{\\boldsymbol{I}})\n",
    "$$\n",
    "\n",
    "where $U$ is the face subspace, $\\bar{\\boldsymbol{I}}$ is the average face learned from the training data and $\\boldsymbol{I}_n$ is the new sample to recognize.\n",
    "\n",
    "Once the sample is in the same subspace as the training samples, we can measure its **similarity** (*distance*) with the centroids $\\boldsymbol{\\Omega}_k$ computed before. The predicted label will be the one corresponding the closest centroid. \n",
    "\n",
    "$$ \n",
    "min \\left|\\left| \\boldsymbol{\\omega}_n - \\boldsymbol{\\Omega}_k \\right|\\right| \\quad \\forall k \\in \\{Train\\}\n",
    "$$\n",
    "\n",
    "Using the prototype below implement a function that predicts each label of new samples.\n",
    "\n",
    "** Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(trsfrm, centroids, centroids_label, samples):\n",
    "    \"\"\"\n",
    "    Perform object recognition on a given list of ``amples\n",
    "\n",
    "    :param trsfrm:          Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param centroids:       List of centroids learned in training phase\n",
    "    :param centroids_label: Label corresponding to the centroids\n",
    "    :param samples:         List of samples to recognize\n",
    "    :return:                Predicted labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Define output container\n",
    "    pred = np.zeros((samples.shape[0], 1), dtype=np.float32)\n",
    "\n",
    "    # Your code goes here ...\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Done\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to validate the implementation is to try the whole system on the training set. The expected recognition accuracy should by close to 100% depending on the task difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize training set\n",
    "pca_train_pred = recognize(pca, train_centroids, train_centroid_label, train_img)\n",
    "\n",
    "# Compute performance\n",
    "n_err = np.count_nonzero(np.where(pca_train_pred != train_label))\n",
    "train_acc = 1.0 - n_err / train_label.size\n",
    "print(\"The recognition accuracy on the trainig set is {:.2f}\".format(train_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the recognition accuracy on the testing set and comments on what you see\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring accuracy of the system is a good indicator of the overall performance but does not indicate where the system is performing poorly. This can be quantified using **Confusion Matrix**. It describes the performance of classification model and shows how the system is confused for each sample in the training set.\n",
    "\n",
    "Such representation can be computed using `sklearn.metrics.confusion_matrix`, details are provided [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "cm = #Code here\n",
    "classes = # Code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_confusion_matrix(cm, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisherface\n",
    "\n",
    "The subspace computed before with *PCA* was looking at directions where the variation in the data is maximum without paying attention to the class each data point belongs to. Therefore this approach is unsupervised. The major drawback is that the class separability is not guaranteed to be optimum. \n",
    "\n",
    "The approach of *Linear Discriminant Analysis* is to find a subspace where the variation is large (*i.e. similar to PCA*) but also to maximize the inter-class separability by taking into account each sample's label. The figure below shows an example:\n",
    "\n",
    "<img src=\"../data/lda_example.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Such subspace can be computed as follow:\n",
    "\n",
    "- Compute the scatter matrices (*intra-classes* / *inter-classes*)\n",
    "- Compute the eigenvectors / eigenvalues\n",
    "- Select the $K_m$ largest eigenvalues and their corresponding eigenvectors\n",
    "\n",
    "Given a set of samples $\\boldsymbol{I}_0, \\dots, \\boldsymbol{I}_N$ and their corresponding labels $y_0, \\dots, y_N$, the intra-class scatter matrix is computed as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_w = \\sum_{i=1}^N \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right) \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right)^{\\top}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mu}_{k}$ is the sample mean of the $k^{th}$ class.\n",
    "Then the inter-class scatter matrix is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b = \\sum_{k=1}^{m} n_k (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})^{\\top}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of classes, $\\boldsymbol{\\mu}$ is the overall sample average and $n_k$ is the number of samples in the $k^{th}$ class.\n",
    "\n",
    "Finally the subspace $\\boldsymbol{W}$ can be computed by solving the following generalizeed eigenvalue problem:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b \\boldsymbol{W} = \\lambda \\boldsymbol{S}_w \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "Finally at most $m-1$ generalized eigenvectors are useful to discriminate between $m$ classes.\n",
    "\n",
    "In practice, such decomposition can be computed using `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, more information available [here](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html).\n",
    "\n",
    "Now compute the *LDA* subspace similar to what you have done before and display the first 8 bases (*i.e. Fisherface*).\n",
    "\n",
    "**Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FisherFaces with LDA\n",
    "n_component = #code here \n",
    "# Compute Eigenfaces\n",
    "lda = #  Code here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basis\n",
    "# Code here\n",
    "\n",
    "\n",
    "# Display first four eigenfaces\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 7))\n",
    "\n",
    "for k in range(8):\n",
    "    fisherface = #Define here.\n",
    "    \n",
    "    # Show image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we have done before visualize the subspace created by the *LDA* decomposition. \n",
    "\n",
    "What do you see, is it better than before ?\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poject data onto subspace\n",
    "proj_train = lda.transform(train_img)\n",
    "\n",
    "# Visualize \n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(proj_train[:, 2], proj_train[:, 3], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what has been done before, compute the recognition accuracy on the *training*/testing* set for the *LDA* recognizer.\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "\n",
    "print(\"The recognition accuracy on the trainig set is {:.2f}\".format(train_acc))\n",
    "\n",
    "\n",
    "print(\"The recognition accuracy on the testing set is {:.2f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the *Confusion Matrix* and comment on the result you have\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "cm = #Define here\n",
    "classes = #Code here; \n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_confusion_matrix(cm, classes, normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have implemented / tested two approaches for face recognition which one works the best and why ? What's are the pro/cons of each method ?\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
