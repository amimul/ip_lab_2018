{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Object Tracking (Face Tracking)\n",
    "\n",
    "## Part 2: Meanshift and Camshift.\n",
    "\n",
    "\n",
    "In this subsection, you will now use _meanshift_ and the _camshift_ algorithms for object tracking. The Meashift algorithm was originally presented as a clustering technique, which was then adapted for object tracking. On the other hand, the \"Continuously  Adaptive  Meanshift\"  algorithm (CamShift) is an extension of the Meanshift algorithm for object tracking in more general situations.\n",
    "\n",
    "MeanShift is nothing but an algorithm for finding modes in a set of data samples representing an underlying probability density function (PDF) in $R^N$. It is a nonparametric clustering technique which does not require prior knowledge of the number of clusters and does not constrain the shape of the clusters.\n",
    "\n",
    "In this section, you will use the OpenCV documentation to apply the aforementioned methods to a video file. Similar to the previous notebook you will use any high level (already implemented) function from OpenCV to complete the exercise specifications and outputs. The aim of this exercise is focused on __evaluating you capacity to search and understand basic image processing functions from OpenCV to complete the task__.\n",
    "\n",
    "\n",
    "__Section Objectives:__\n",
    "\n",
    "* Understand the use of the Meanshift and Camshift algorithm using OpenCV for face tracking.\n",
    "\n",
    "* Apply them to video data and extract the results.\n",
    "\n",
    "* Compare and evaluate the performance between them and a given ground truth\n",
    "\n",
    "__Data__\n",
    "\n",
    "For this assignment, you will use the data videos on ``../data/videos/`` labelled as \"video_girl.avi\". The ground truth (the solution or gold standard of the exercise) is included in the file: \"video_girl_groundtruth_rect.txt\". You will later use these results to compute the accuracy of your tracking. \n",
    "\n",
    "\n",
    "__Groundtruth__\n",
    "\n",
    "The ground truth contains a list of four numbers per frame. Each line lists the __row, column, width and height__ of the face position.\n",
    "\n",
    "\n",
    "\n",
    "## Meanshift \n",
    "\n",
    "\n",
    "Meanshift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. It was originally presented in 1975 by Fukunaga and Hostetler paper, as a clustering algorithm. The mean shift algorithm can be used for visual tracking. The simplest such algorithm would create a confidence map in the new image based on the colour histogram of the object in the previous image and use Meanshift to find the peak of a confidence map near the object's old position. OpenCV provides a useful tutorial and example in its official documentation: \n",
    "\n",
    "https://docs.opencv.org/3.4.1/db/df8/tutorial_py_meanshift.html\n",
    "\n",
    "\n",
    "As the first exercise, you will use the Meanshift function inside OpenCV to do face tracking. You will use your previously defined function (from the first assignment) to apply the Meanshift function over the first __50 frames__ of the video \"video_girl.avi\" and save the coordinates of the __region detected by the algoritmh__. As you can see (once you read the documentation and the example), mean-shift requires an initial mask region to perform the following tracking; you will use the first region listed in the ground truth file as an initial region.\n",
    "\n",
    "\n",
    "Specifications: \n",
    "\n",
    "* The implemented solution should use the same function implemented in the first part.\n",
    "* The output should contain a list of the rectangle region obtained from Meanshift over the first 50 frames.\n",
    "* The function should receive as the parameter all the parameters needed to call the Meanshift function over the video.  This means that a user shouldn't need to modify the function internal code to test any set of parameters. \n",
    "* To corroborate your results, display the frame with the output rectangle region drawn in it, for the frames 1, 25 and 50.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_frame_function(image,params):\n",
    "    # Mean shift calls and steps\n",
    "    # Region extraction \n",
    "    return [] # Output region for the first 50 frames.\n",
    "\n",
    "file_path =  os.path.join('..','data', 'videos', 'video_girl.avi')\n",
    "\n",
    "all_the_output = meanShiftFromVideoFile(file_path, process_frame_function, [\"All\",\"you\",\"may\",\"need\"]);\n",
    "\n",
    "## Be sure to display the results below:\n",
    "\n",
    "## Frame number\n",
    "## Display image frame (1, 25, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints \n",
    "\n",
    "* As mentioned before, this exercise will evaluate your capacity to understand and adapt the example code from the aforementioned tutorial to solve the exercise. However, feel free to search and use any other example/code that you may find.\n",
    "* Be sure to understand each line from the suggested example. Some parts you should pay attention to are:\n",
    "  * What are the hardcoded values given to the functions: inRange, calcHist and calcBackProject? \n",
    "  * Are those the best values for my input? \n",
    "  * How does the mask looks like? \n",
    "* Notice that the ground truth contains the row, column, width, height. In the example, they have different input order.\n",
    "* Finally, notice that in this tutorial they use the HSV colour space, specifically the first channel. Why is this? \n",
    "\n",
    "\n",
    "\n",
    "## Camshift\n",
    "\n",
    "Continuously adaptive mean-shift(CAMShift) was proposed as an efficient and light-weight tracking algorithm developed based on mean-shift. What Camshift do is nothing but do meanShift in every single frame of a video, and adapt the parameters and the window using (a very basic) backpropagation. With this simple idea, however, Camshift is able to track objects in multiscale situations. On the other hand, it makes Camshift much more sensible to lightning changes or obfuscation problems. \n",
    "\n",
    "\n",
    "As a second exercise, you will test the Camshift algorithm from OpenCV following the same specifications to the same first __50 initial frames__. You can use a different set of parameters to test, the main shift and make it work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_frame_function(image,params):\n",
    "    # Mean shift calls and steps\n",
    "    # Region extraction \n",
    "    return [] # Output region for the first 50 frames.\n",
    "\n",
    "file_path =  os.path.join('..','data', 'videos', 'video_girl.avi')\n",
    "\n",
    "all_the_output = camShiftFromVideoFile(file_path, process_frame_function, [\"All\",\"you\",\"may\",\"need\"]);\n",
    "\n",
    "## Be sure to display the results below:\n",
    "\n",
    "## Frame number\n",
    "## Display image frame (1, 25, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision metrics\n",
    "\n",
    "For the last part of the assignment, you are asked to compare the performance of the Camshift tracker and the Meanshift tracker, frame wise, in terms of average Intersection over Union (IoU) of the object ground truth bounding boxes during the first __50 frames__; which is basically the ratio between the area of the intersection over the union.\n",
    "\n",
    "You can find the metric as the Jaccard index https://en.wikipedia.org/wiki/Jaccard_index. Feel free to modify the implementation below to adapt it to your function output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820359281437125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def IoU(boxA, boxB):\n",
    "    \n",
    "    '''\n",
    "    Inputs: \n",
    "        boxA: top left and bottom right coordinates of the first bunding Box\n",
    "        boxB: top left and bottom right coordinates of the second bunding Box\n",
    "\n",
    "    '''\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = (xB - xA + 1) * (yB - yA + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "IoU([39, 63, 203, 112],[40, 63, 205, 112])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the following quantities: \n",
    "\n",
    "* The Min, Mean and Max IoU between the Meanshift and the ground truth (first 50 frames).\n",
    "* The Min, Mean and Max IoU between the Camshift and the ground truth (first 50 frames).\n",
    "* Display the BEST (min IoU) and the worst (max IoU) for both algorithms.\n",
    "\n",
    "__REMARKS:__ If any of the two algorithms is disastrously failing you need to go back and tune the parameters. If the tracking is unsuccessful due to an inappropriate adaptation/implementation of the algorithms, no points will be given.\n",
    "\n",
    "\n",
    "Write a small report summarizing the election of the selected parameters. Mention any important remark over the performance, especially indicating why it is failing in some cases (unless you have a perfect IoU). \n",
    "\n",
    "Finally, make a **quantitative** comparison of the performance of both methods. Include and display any frame where you consider that the difference between both methods manifests.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
