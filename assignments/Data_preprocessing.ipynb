{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Deep Learning (Part 1) - Data preprocessing\n",
    "\n",
    "\n",
    "Part 1\n",
    "------------\n",
    "\n",
    "\n",
    "The objective of this first part is to learn about simple data curation practices. Data curation (for machine learning) consists basically in analyse, label, and separate in classes your input data.  In section two, you used pre-curated and separated data from the INRIA's person data set. Your first task is to create your training/testing sets by hand and analyse how well \"balance\" they are. \n",
    "\n",
    "**Objectives** \n",
    "In the following sections, you will use this part to \"feed\" both, a classic _Swallow_ (not-Deep) classification using handcrafted features and a Deep (kind of Deep) neuronal network. Your task will consist in analyse the accuracy of the aforementioned classification based on the amount of data available; from few hundred of samples to the full data-set.\n",
    "\n",
    "In this section we will provide the general steps and, as in the previous section, you will be asked to search in the function parameters and syntaxis in users documentation.\n",
    "\n",
    "\n",
    "## Dataset \n",
    "\n",
    "In this last section of the course we will use the the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like _real data_: it's not trivial, and the data is a lot less 'clean' than MNIST.\n",
    "\n",
    "\n",
    "### Libraries: \n",
    "\n",
    "Be sure that you can import all the libraries below, in addition, for next chapter you will make use of the tensor flow library to implement the used neuronal network. Be sure to be able to import it as: \n",
    "\n",
    "`` import tensorflow as tf``\n",
    "\n",
    "following the documentation page \n",
    "\n",
    "https://www.tensorflow.org/install/\n",
    "\n",
    "We will use only CPU's based training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "from utils import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data. \n",
    "\n",
    "As in section 2, you need to download the data and set the input directory. Be sure to have at about ~1 Gb of free space. If the function is not able to download the data, try on the MNIST site. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the Data\n",
    "# The following functions will download the data for you and uncompress it\n",
    "\n",
    "# WARNING:  These varaibles set the input/output paths for ALL the bellow functions.\n",
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '../data/' # Change me to store data elsewhere\n",
    "\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"\n",
    "  Downloads a file if not present, and make sure it's the right size!.\n",
    "  If there's a file with the same name, the function will not try to \n",
    "  download the dataset again!\n",
    "  \"\"\"\n",
    "\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "    \n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename, 'This may take a while. Please wait.') \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'The file ' + dest_filename + 'already exist but seems corrupted. Delete it or download it from the browser!')\n",
    "  return dest_filename\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  \"\"\"\n",
    "  Uncompress the data set for you\n",
    "  \"\"\"\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s dataset (seems to be) already present.\\nSkipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "    \n",
    "  data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))]\n",
    "  print(\"All setup.\")\n",
    "  return data_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads ifa needed.\n",
    "large_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "small_filename  = maybe_download('notMNIST_small.tar.gz', 8458043)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_folders = maybe_extract(large_filename)\n",
    "small_folders = maybe_extract(small_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "When working with data, always check your data. \n",
    "\n",
    "Create a description of your input data. Describe in a table or list (one for each sample size): \n",
    "\n",
    "* Number of classes (characters)\n",
    "* Number of samples per class\n",
    "* General information on the image size and number of channels.\n",
    "\n",
    "Visualize one sample per class bellow for a chosen size (large or small). \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you have all your images set up we will load the data into a more manageable format. Since depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. \n",
    "\n",
    "To do this we will use pickles!\n",
    "\n",
    "https://docs.python.org/3.2/library/pickle.html\n",
    "\n",
    "“Pickling” is the process whereby a Python object (it can be anything!) is converted into a byte stream (binary format), and “unpickling” is the inverse operation. We will use pickles to save the FULL set of images for each character in one pickle. The result will be a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. This process is known as \"normalizing the data\" or \"feature scaling, which is very important to ensure convergence in the optimization step, as well to ensure that the feature space is well defined.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feature_scaling\n",
    "\n",
    "\n",
    "Your task: using the skeleton function bellow you need to: \n",
    "\n",
    "* 1) Load all the images in FLOAT format for each class (A,..,J), 1 channel only.\n",
    "* 2) Transform each image intensities such that the range goes from -125,125 (instead of 0, 256)\n",
    "* 3) Scale the function so the new range goes from -0.5 to 0.5.\n",
    "\n",
    "A few images might not be readable, we'll just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Image fixed size  \"\"\"\n",
    "image_size  = 28     # Pixel width and height. (28x28)\n",
    "pixel_depth = 255.0  # Number of levels per pixel. (0,255)\n",
    "\n",
    "\"\"\" There's should be enough data at the end\"\"\"\n",
    "min_num_images_train = 45000;\n",
    "min_num_images_test  = 1800;\n",
    "\n",
    "\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \n",
    "  \"\"\" Base function: \n",
    "  \n",
    "      Complete this function to read a each iamge of a given character (folder)\n",
    "      Transforms and scale the image to have 0 mean and standard deviation of ~0.5.\n",
    "      \n",
    "      Params: \n",
    "          folder: input character folder (e.g. ../data/notMNIST_large/A/)\n",
    "          min_num_images: minimum number of images you should have per character.\n",
    "      \n",
    "      returns: \n",
    "          dataset: Vector containing the fully loaded and scaled dataset.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  image_files = os.listdir(folder)\n",
    "    \n",
    "  # Array size (should be preserved)  \n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "\n",
    "  # List of all the images inside the folder  \n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    \n",
    "  # If the image is not loadable (there are some corrupted images you can skip them) \n",
    "    try:\n",
    "        \n",
    "      # CODE HERE:\n",
    "    \n",
    "      #Load each image and transform them\n",
    "      \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #--- End of your code.\n",
    "    \n",
    "      # here I check that you load them correctly and save it in the dataset array.  \n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "    \n",
    "      num_images = num_images + 1\n",
    "        \n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "\n",
    "  # If this theshhold is not met, you are doind something wrong (probably)  \n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    " \n",
    "  #Check this output! \n",
    "  # The mean shoudl be very close to 0 i.e < 1 and the std should be less than 0.5.  \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  # Notice we aree callign this a \"tensor\"\n",
    "\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look, Morty, I'm a Pickle!        \n",
    "\n",
    "# This function calls your pre-defined-function load_letter(folder, min_num_images) and creates the pickle!\n",
    "\n",
    "def Im_a_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    \n",
    "    \"\"\" Base function: \n",
    "  \n",
    "      Loads all the images listed in data_folders and creates a .pickle file\n",
    "      \n",
    "      Params: \n",
    "          data_folders: list of the folders to pickle (i.e. large_folders, small_folders)\n",
    "          min_num_images: minimum number of images you should have per character.\n",
    "      \n",
    "      returns: \n",
    "          dataset_names: Vector containing all the pickles names.\n",
    "  \"\"\"\n",
    "    dataset_names = []\n",
    "\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "    \n",
    "        if os.path.exists(set_filename) and not force:\n",
    "          # You may override by setting force=True.\n",
    "          print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "          print('Turning myself into a Pickle! %s.' % set_filename)\n",
    "\n",
    "          dataset = load_letter(folder, min_num_images_per_class)\n",
    "\n",
    "          try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "              pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "          except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was done correctly we can then call the following functiosn without error!\n",
    "\n",
    "Notice that we are here considereing the \"large\" data set as our training data set and the small as our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = Im_a_pickle(large_folders, 45000)\n",
    "test_datasets  = Im_a_pickle(small_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2\n",
    "\n",
    "---------\n",
    "\n",
    "To corroborate that our data is properly saved and scaled, display one example per class letter (A,...,J), from the train dataset **or** the test dataset. \n",
    "\n",
    "To do this you will need to use the ``pickle.load(...)``. Check the documentation above for more details. You can use the inhered matplotlib function to show each example. Include a colorbar showing the __values range of the image__.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally if everything is correct, the above function should contain the full length of each character sample.  The labels will be stored into a separate array of *integers 0 through 9*.\n",
    "\n",
    "Corroborate that the train_sets are in the order of ~52,000 images, and the train_set in the order of 1,870 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_sets_sizes(data_set):\n",
    "    \n",
    "    number_files = [];\n",
    "\n",
    "    \"\"\" Base function: \n",
    "  \n",
    "      Loads all the images listed in data_set and return it' s size\n",
    "      \n",
    "      Params: \n",
    "          data_folders: lsit of the folders to pickle (i.e. large_folders, small_folders)\n",
    "          min_num_images: minimum number of images you should have per character.\n",
    "      \n",
    "      returns: \n",
    "          dataset_names: Vector containing all the pickles names.\n",
    "  \"\"\"\n",
    "        \n",
    "    #Code here \n",
    "\n",
    "    print(number_files)\n",
    "    \n",
    " \n",
    "data_sets_sizes(train_datasets)\n",
    "\n",
    "data_sets_sizes(test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 \n",
    "### Creating sub-sampled datasets.\n",
    "\n",
    "\n",
    "In order to evaluate the performance of our classifiers, we need to create subsets of our data properly randomized; this means that we shouldn't choose always the first set of images to compare since we will introduce a bias because of the sampled order. A very nice post on this topic can be found below in case you wonder if it's worth the trouble.\n",
    "\n",
    "https://machinelearningmastery.com/randomness-in-machine-learning/\n",
    "\n",
    "\n",
    "As problem 3 you are asked to write a function: ``sample_training_data(...)`` which should create a training dataset of a given size, containing the same number of samples for each label (-1 or +1 samples) _randomly selected_ from the ``train-dataset``; as well as the labels of the training set coded as integers from 0 (A) to 9 (J). \n",
    "\n",
    "\n",
    "Is worth mentioning that is common practice in machine learning to set aside a third data set known as the _validation dataset_. Which is used to prevent overfitting and other training problems. We will not make use of such dataset, however, is worth to check why is used; one nice and short explanation can be found here (it also contains nice code hints relevant to the exercise ;) )\n",
    "\n",
    "https://machinelearningmastery.com/difference-test-validation-datasets/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Code here.\n",
    "def sample_training_data(pickle_files, train_size):\n",
    "\n",
    "    \"\"\" Base function: \n",
    "  \n",
    "      Given a train size returns a ndarray containing a total of \n",
    "      <train_size/number_of_clases> samples for each character.\n",
    "      \n",
    "      example for train_size = 100, should contain 100/10 = 10 samples of each character. \n",
    "      \n",
    "      The samples should be chose randomly.\n",
    "      \n",
    "      Params: \n",
    "          pickle_files: list of the pickle files (training set)\n",
    "          train_size: total length of the new training set\n",
    "      \n",
    "      returns: \n",
    "          train_dataset: ndarray containing all the trainign images (properly normalized)\n",
    "          train_labels : the labels of each selected image. \n",
    "  \"\"\"\n",
    "    \n",
    "    train_dataset = []\n",
    "    train_labels = []\n",
    "    \n",
    "    return train_dataset, train_labels\n",
    "  \n",
    "\n",
    "# EXAMPLE OF USE\n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "print('Training size: ', train_dataset.shape, '\\nLabel vector size:',train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you need to randomize the vector so it doesn't follow any specific order: like first all the A characters and then the 'B' characters, and so on \n",
    "\n",
    "```(A, A, ..., A , B, B, ..., B, C, C, ...,C,... )```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to randomize THE ORDER of a given dataset.\n",
    "# Be sure that the dataset and the labels are shuffled in the same order so they MATCH.\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    #code here\n",
    "\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels   = randomize(test_dataset, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Show us that your method works and both datasets are coherent with the labels. You can display the shuffled order and show the first images in both datasets. They should match the labels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 5\n",
    "---------\n",
    "\n",
    "By default, this dataset might contain a lot of overlapping samples, including training data that are also contained in the test set. The overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap but are actually ok if you expect to see training samples recur when you use it.\n",
    "\n",
    "Measure how much overlap there is between training, validation and test samples. \n",
    "\n",
    "- Give the number of overlapping samples in the test set for the full dataset.\n",
    "- What about near duplicates between datasets? (images that are almost identical), provide a __estimation__ based in any metric or function to evaluate similitude with a given threshold.  \n",
    "- Modify your ``sample_training_data``function and provide a curated train and test dataset removing the very similar samples from one of them. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 6\n",
    "---------\n",
    "\n",
    "Let's get an idea of what a basic classifier can give you on this data. \n",
    "\n",
    "Train a simple model on this data using 50, 100, 1000 and 5000 training samples. \n",
    "\n",
    "Hint: you can use the ```LogisticRegression``` or ```LogisticRegressionCv``` model from sklearn.linear_model.\n",
    "\n",
    "Provide a score for the prediction over the full test data set. You can use any metric from the previous chapters or an implemented one like the ```cross_val_score``` form sklearn which is more accurate.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "A good choice of parameters (and regularization method) can give you results up to the 89%\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "#Samples: 50 ---> Score: 0.471428571429\n",
    "#Samples: 100 ---> Score: 0.605865717935\n",
    "#Samples: 1000 ---> Score: 0.760772183027\n",
    "#Samples: 5000 ---> Score: 0.812826972435 \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "def train_and_validate(num_examples):\n",
    "    # code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [50, 100, 1000, 5000]\n",
    "\n",
    "print(\"Cross Validation Score\\n\")\n",
    "for size in training_sizes:\n",
    "    score = train_and_validate(size)\n",
    "    print(\"Samples:\", size,\"---> Score:\", score)\n",
    "    \n",
    "    \n",
    "#Samples: 50 ---> Score: 0.471428571429\n",
    "#Samples: 100 ---> Score: 0.605865717935\n",
    "#Samples: 1000 ---> Score: 0.760772183027\n",
    "#Samples: 5000 ---> Score: 0.812826972435\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
